# JSON MCP Configuration - Complete Setup & Demo

## üéØ What You'll Build

A simple JSON-based MCP configuration that automatically routes questions to the right models without any programming.

---

## üìö **Theory: Understanding Ollama & Large Language Models**

### **ü§ñ What is Ollama?**

**Ollama** is a lightweight, extensible framework for building and running large language models (LLMs) locally on your machine. Think of it as "Docker for AI models" - it makes running powerful AI models as simple as running a container.

#### **Key Benefits of Ollama:**
- **üè† Local Execution** - Models run on your hardware, no cloud dependency
- **üîí Privacy** - Your data never leaves your machine
- **‚ö° Fast** - No network latency, instant responses
- **üí∞ Cost-Effective** - No API fees or usage limits
- **üõ†Ô∏è Developer-Friendly** - Simple REST API for integration

#### **How Ollama Works:**
```
Your Question ‚Üí Ollama Server ‚Üí Selected Model ‚Üí Generated Response
     ‚Üì              ‚Üì              ‚Üì              ‚Üì
"Plan a trip"  ‚Üí localhost:11434 ‚Üí phi-fast ‚Üí "Visit Eiffel Tower..."
```

### **üß† Understanding Large Language Models (LLMs)**

#### **What are LLMs?**
Large Language Models are AI systems trained on vast amounts of text data to understand and generate human-like text. They're like incredibly sophisticated autocomplete systems that can:

- **Understand Context** - Grasp what you're asking about
- **Generate Responses** - Create coherent, relevant answers
- **Reason** - Apply logic and knowledge to solve problems
- **Specialize** - Excel in specific domains (coding, writing, analysis)

#### **Model Specialization (Why MCP Matters):**

Different models are trained for different purposes:

| Model Type | Training Focus | Best For | Limitations |
|------------|----------------|----------|-------------|
| **General Purpose** | Broad knowledge, conversation | Travel, general questions, explanations | May lack deep technical expertise |
| **Code Specialists** | Programming languages, algorithms | Writing code, debugging, technical docs | Often refuse non-coding questions |
| **Domain Experts** | Specific fields (medical, legal, etc.) | Specialized knowledge tasks | Limited outside their domain |

#### **The Model Selection Problem:**
```
Student Question: "Plan a 3-day trip to Paris"

‚ùå Wrong Model Choice (deepseek-coder):
   "I'm sorry, I'm designed for programming questions only"

‚úÖ Right Model Choice (phi-fast):
   "Here's a great 3-day Paris itinerary: Day 1: Eiffel Tower..."
```

**This is exactly why MCP exists - to automatically choose the right model for each question!**

### **üîÑ The MCP Solution**

#### **Model Context Protocol (MCP) Concept:**
MCP is like having an intelligent assistant that:
1. **Listens** to your question
2. **Analyzes** what type of help you need
3. **Selects** the best-suited AI model
4. **Coordinates** the interaction
5. **Delivers** the most helpful response

#### **Our Demo Models:**

<details>
<summary><strong>üìä phi-fast:latest - General Purpose Model</strong></summary>

**What it is:** Microsoft's Phi model optimized for speed and general tasks

**Strengths:**
- Fast response times
- Good general knowledge
- Handles travel, lifestyle, general questions well
- Balanced performance across topics

**Best for:**
- Travel planning
- General advice
- Explanations of concepts
- Creative writing
- Casual conversations

**Limitations:**
- Not specialized for complex coding
- May lack deep technical expertise
- Limited in highly specialized domains

**Example Usage:**
```bash
# Perfect for travel questions
python3 smart_chat.py "What's the best time to visit Japan?"
# Result: Detailed seasonal advice, weather info, cultural events
```
</details>

<details>
<summary><strong>üíª deepseek-coder-fast:latest - Programming Specialist</strong></summary>

**What it is:** DeepSeek's specialized coding model optimized for programming tasks

**Strengths:**
- Excellent code generation
- Understands multiple programming languages
- Good at debugging and code explanation
- Follows coding best practices

**Best for:**
- Writing functions and algorithms
- Code debugging and optimization
- Programming tutorials
- Technical documentation
- Software architecture advice

**Limitations:**
- **Refuses non-coding questions** (this is intentional design)
- May be overkill for simple tasks
- Less conversational than general models

**Example Usage:**
```bash
# Perfect for coding questions
python3 smart_chat.py "Write a Python function to sort a dictionary by values"
# Result: Clean, well-commented code with explanations

# But fails for non-coding questions
curl -d '{"model": "deepseek-coder-fast:latest", "prompt": "Plan a vacation"}' ...
# Result: "I'm designed for programming questions only"
```
</details>

<details>
<summary><strong>üéì mistral:latest - Educational & Explanatory Model</strong></summary>

**What it is:** Mistral AI's model designed for clear explanations and educational content

**Strengths:**
- Excellent at breaking down complex topics
- Clear, structured explanations
- Good for learning and education
- Balanced and informative responses

**Best for:**
- Explaining concepts ("What is machine learning?")
- Educational content
- How-to guides
- Research assistance
- Analytical tasks

**Limitations:**
- May be verbose for simple questions
- Not specialized for coding or creative tasks
- General purpose rather than domain expert

**Example Usage:**
```bash
# Perfect for educational questions
python3 smart_chat.py "Explain how neural networks work"
# Result: Clear, step-by-step explanation with examples
```
</details>

---

## üìã Prerequisites Setup

### üê≥ Docker Installation

**Why Docker?** Docker provides a consistent, isolated environment for running Ollama across different operating systems. It eliminates "works on my machine" problems and ensures everyone has the same setup.

<details>
<summary><strong>üêß Ubuntu/Debian Systems</strong></summary>

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y
```
**What this does:** Updates your package list and upgrades existing packages to latest versions

```bash
# Install Docker and Docker Compose
sudo apt install -y docker.io docker-compose curl
```
**What this does:** 
- `docker.io` - The Docker engine for running containers
- `docker-compose` - Tool for defining multi-container applications
- `curl` - Command-line tool for making HTTP requests (needed for testing)

```bash
# Start Docker service
sudo systemctl start docker
sudo systemctl enable docker
```
**What this does:**
- `start docker` - Starts the Docker daemon immediately
- `enable docker` - Ensures Docker starts automatically on system boot

```bash
# Add current user to docker group
sudo usermod -aG docker $USER
```
**What this does:** Adds your user to the `docker` group so you can run Docker commands without `sudo`

```bash
# Apply group changes (logout/login or use newgrp)
newgrp docker
```
**What this does:** Applies the group membership change immediately without requiring logout/login

```bash
# Verify Docker installation
docker --version
docker run hello-world
```
**What this does:**
- Shows Docker version to confirm installation
- Runs a test container to verify Docker is working properly
- **Expected output:** Version info and "Hello from Docker!" message
</details>

<details>
<summary><strong>üé© CentOS/RHEL/Fedora Systems</strong></summary>

```bash
# Install Docker
sudo dnf install -y docker docker-compose
```
**What this does:** Uses DNF package manager (newer than YUM) to install Docker components

```bash
# Start Docker service
sudo systemctl start docker
sudo systemctl enable docker
```
**What this does:** Same as Ubuntu - starts Docker now and on boot

```bash
# Add user to docker group
sudo usermod -aG docker $USER
newgrp docker
```
**What this does:** Same user permission setup as Ubuntu

```bash
# Verify installation
docker --version
docker run hello-world
```
**What this does:** Confirms Docker is working correctly
</details>

<details>
<summary><strong>üçé macOS Systems</strong></summary>

**Option 1: Docker Desktop (Recommended)**
```bash
# Download from: https://docs.docker.com/desktop/mac/install/
# Install the .dmg file and start Docker Desktop
```

**Option 2: Using Homebrew**
```bash
# Install Docker Desktop via Homebrew
brew install --cask docker
```
**What this does:** Uses Homebrew package manager to install Docker Desktop

```bash
# Start Docker Desktop application
# Then verify in terminal:
docker --version
```
**What this does:** Confirms Docker Desktop is running and accessible from terminal
</details>

<details>
<summary><strong>ü™ü Windows Systems</strong></summary>

**Option 1: Docker Desktop (Recommended)**
```bash
# Download from: https://docs.docker.com/desktop/windows/install/
# Run the installer and restart when prompted
```

**Option 2: Using Package Managers**
```bash
# Using Chocolatey
choco install docker-desktop

# Using winget
winget install Docker.DockerDesktop
```

```bash
# Verify in PowerShell/CMD:
docker --version
```
**What this does:** Confirms Docker is installed and accessible from command line
</details>
</details>

### ü§ñ Ollama Setup (Docker Method)

**Why Ollama in Docker?** Running Ollama in Docker ensures consistent behavior across all systems and makes it easy to manage models and dependencies.

<details>
<summary><strong>üöÄ Run Ollama Container</strong></summary>

```bash
# Pull and run Ollama Docker container
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

**Command Breakdown:**
- `docker run` - Creates and starts a new container
- `-d` - Runs container in detached mode (background)
- `--name ollama` - Names the container "ollama" for easy reference
- `-p 11434:11434` - Maps port 11434 from container to host (Ollama's default port)
- `-v ollama:/root/.ollama` - Creates persistent volume for model storage
- `ollama/ollama` - The official Ollama Docker image

**What happens:**
1. Docker downloads the Ollama image (if not already present)
2. Creates a container named "ollama"
3. Starts Ollama server on port 11434
4. Sets up persistent storage for AI models

```bash
# Verify it's running
docker ps
```
**What this shows:** List of running containers - you should see "ollama" container

```bash
curl http://localhost:11434/api/tags
```
**What this does:** Tests if Ollama API is responding
**Expected output:** JSON response with available models (initially empty)
</details>
</details>

### üì• Download Required AI Models

**Understanding Model Downloads:** AI models are large files (1-7GB each) containing the neural network weights trained on massive datasets. Ollama downloads and manages these models for you.

<details>
<summary><strong>üîΩ Download Models via Docker</strong></summary>

```bash
# Download lightweight models for the demo
docker exec -it ollama ollama pull phi-fast:latest
```
**What this does:**
- `docker exec -it ollama` - Executes command inside the running Ollama container
- `ollama pull phi-fast:latest` - Downloads the phi-fast model (latest version)
- **Size:** ~2.7GB - General purpose model, good for travel/general questions
- **Download time:** 5-15 minutes depending on internet speed

```bash
docker exec -it ollama ollama pull deepseek-coder-fast:latest
```
**What this does:**
- Downloads DeepSeek Coder model optimized for programming tasks
- **Size:** ~3.8GB - Specialized for code generation and debugging
- **Key feature:** Refuses non-coding questions (demonstrates model specialization)

```bash
docker exec -it ollama ollama pull mistral:latest
```
**What this does:**
- Downloads Mistral model for educational and explanatory content
- **Size:** ~4.1GB - Excellent for clear explanations and learning
- **Best for:** "Explain how X works" type questions

```bash
# Verify models are downloaded
docker exec -it ollama ollama list
```
**What this shows:**
- List of all downloaded models with sizes and IDs
- **Expected output:** Three models listed with their details
- **Total storage:** ~10GB for all three models
</details>

<details>
<summary><strong>üß™ Test Models Are Working</strong></summary>

```bash
# Test general model
docker exec -it ollama ollama run phi-fast:latest "Hello, can you help with travel planning?"
```
**What this does:**
- `ollama run` - Starts interactive session with specified model
- Tests if phi-fast can handle travel-related questions
- **Expected response:** Friendly greeting and confirmation it can help with travel

```bash
# Test code model
docker exec -it ollama ollama run deepseek-coder-fast:latest "Write a simple Python function"
```
**What this does:**
- Tests deepseek-coder's programming capabilities
- **Expected response:** A simple Python function with explanation

```bash
# Quick API test
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-fast:latest",
    "prompt": "Say hello",
    "stream": false
  }'
```
**What this does:**
- Tests Ollama's REST API directly
- `POST /api/generate` - Main endpoint for text generation
- `"stream": false` - Gets complete response at once (not streaming)
- **Expected response:** JSON with generated text in "response" field

**API Response Format:**
```json
{
  "model": "phi-fast:latest",
  "created_at": "2024-01-01T12:00:00Z",
  "response": "Hello! How can I help you today?",
  "done": true
}
```
</details>
</details>

### ‚úÖ Quick Verification

**System Health Check:** These commands verify that your entire MCP demo environment is ready.

<details>
<summary><strong>üîç Step-by-Step Verification Commands</strong></summary>

```bash
# 1. Check Docker is running
docker ps
```
**What this checks:** Docker daemon is active and can list containers
**Expected output:** Table showing running containers (should include "ollama")
**If fails:** Run `sudo systemctl start docker`

```bash
# 2. Check Ollama container is running  
docker ps | grep ollama
```
**What this checks:** Ollama container is specifically running and healthy
**Expected output:** Line showing ollama container with "Up" status
**If fails:** Container may have stopped - restart with `docker start ollama`

```bash
# 3. Check Ollama API is responding
curl http://localhost:11434/api/tags
```
**What this checks:** Ollama web server is accepting HTTP requests
**Expected output:** JSON response with model list
**If fails:** Port may be blocked or service not started

```bash
# 4. Check models are available
docker exec ollama ollama list
```
**What this checks:** All required models are downloaded and accessible
**Expected output:** List showing phi-fast, deepseek-coder-fast, and mistral models
**If fails:** Re-run model download commands

```bash
# 5. Test a model works end-to-end
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "phi-fast:latest", "prompt": "Hello", "stream": false}'
```
**What this checks:** Complete pipeline from API request to model response
**Expected output:** JSON with "response" field containing model's greeting
**If fails:** Check model name spelling and container resources
<img width="638" height="988" alt="image" src="https://github.com/user-attachments/assets/06001758-641a-44e5-8468-18f469da74d5" />

**‚úÖ Success Criteria:** All 5 commands return successful responses without errors

**üö® If any command fails:** Check the troubleshooting section below
</details>
</details>

---

## üìÅ Files Overview

- **`smart-mcp.json`** - MCP routing configuration with smart model selection rules
- **`smart_chat.py`** - CLI script that demonstrates automatic model routing

---

## üöÄ How to Use

### **Step 1: Review the Configuration**

<details>
<summary><strong>üìã Examine MCP Configuration</strong></summary>

```bash
# Look at your MCP routing setup
cat smart-mcp.json
```
**What you'll see:** JSON configuration defining routing rules
**Key sections:**
- `routing_rules` - Maps question types to models
- `keywords` - Trigger words for each category
- `use_model` - Which model to select
- `avoid_model` - Which model to avoid

**Example routing rule:**
```json
"travel_questions": {
  "keywords": ["trip", "travel", "plan", "visit", "vacation"],
  "use_model": "phi-fast:latest",
  "avoid_model": "deepseek-coder-fast:latest"
}
```

```bash
# Check the smart chat script
cat smart_chat.py
```
**What you'll see:** Python script that implements MCP logic
**Key functions:**
- `load_routing_config()` - Reads JSON configuration
- `analyze_question()` - Matches keywords to categories
- `call_ollama()` - Makes API calls to selected model
</details>

### **Step 2: Test Smart Model Selection**

**Understanding the Tests:** Each test demonstrates how MCP analyzes question keywords and routes to the appropriate specialized model.

<details>
<summary><strong>üó∫Ô∏è Travel Question Test</strong></summary>

```bash
python3 smart_chat.py "What's the best time to visit Paris?"
```

**What happens internally:**
1. **Keyword Analysis:** Detects "visit" and "Paris" 
2. **Category Match:** Identifies as "travel_questions"
3. **Model Selection:** Routes to phi-fast:latest
4. **API Call:** Sends request to Ollama
5. **Response:** Returns travel advice

**Expected Output:**
```
ü§ñ Smart Chat - Automatic Model Selection
==================================================
üìù Question: What's the best time to visit Paris?

üß† Smart Analysis:
   ‚úì Category: travel_questions
   ‚úì Selected Model: phi-fast:latest
   ‚úì Reason: Detected travel keywords: ['visit']

üîÑ Calling phi-fast:latest...
‚úÖ Smart Response:
--------------------
The best time to visit Paris is during late spring (April-June) or early fall (September-October)...

‚è±Ô∏è  Response time: 2.3s
```

**Why this works:** phi-fast is trained on general knowledge including travel information
</details>

<details>
<summary><strong>üíª Coding Question Test</strong></summary>

```bash
python3 smart_chat.py "Write a Python function to sort a list"
```

**What happens internally:**
1. **Keyword Analysis:** Detects "function" and "Python"
2. **Category Match:** Identifies as "coding_questions"  
3. **Model Selection:** Routes to deepseek-coder-fast:latest
4. **Specialized Processing:** Uses programming-optimized model
5. **Code Generation:** Returns well-structured code

**Expected Output:**
```
ü§ñ Smart Chat - Automatic Model Selection
==================================================
üìù Question: Write a Python function to sort a list

üß† Smart Analysis:
   ‚úì Category: coding_questions
   ‚úì Selected Model: deepseek-coder-fast:latest
   ‚úì Reason: Detected coding keywords: ['function']

üîÑ Calling deepseek-coder-fast:latest...
‚úÖ Smart Response:
--------------------
def sort_list(input_list, reverse=False):
    """
    Sorts a list in ascending or descending order.
    
    Args:
        input_list (list): The list to be sorted
        reverse (bool): If True, sort in descending order
    
    Returns:
        list: A new sorted list
    """
    return sorted(input_list, reverse=reverse)

# Example usage:
numbers = [3, 1, 4, 1, 5, 9, 2, 6]
sorted_numbers = sort_list(numbers)
print(sorted_numbers)  # Output: [1, 1, 2, 3, 4, 5, 6, 9]

‚è±Ô∏è  Response time: 1.8s
```

**Why this works:** deepseek-coder is specifically trained on programming tasks
</details>

<details>
<summary><strong>üéì General Question Test</strong></summary>

```bash
python3 smart_chat.py "Explain what machine learning is"
```

**What happens internally:**
1. **Keyword Analysis:** Detects "explain" 
2. **Category Match:** Identifies as "general_questions"
3. **Model Selection:** Routes to mistral:latest
4. **Educational Processing:** Uses explanation-optimized model
5. **Structured Response:** Returns clear, educational content

**Expected Output:**
```
ü§ñ Smart Chat - Automatic Model Selection
==================================================
üìù Question: Explain what machine learning is

üß† Smart Analysis:
   ‚úì Category: general_questions
   ‚úì Selected Model: mistral:latest
   ‚úì Reason: Detected general keywords: ['explain']

üîÑ Calling mistral:latest...
‚úÖ Smart Response:
--------------------
Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed for every task.

Key Concepts:
1. **Training Data**: ML systems learn from examples (data)
2. **Algorithms**: Mathematical methods that find patterns in data
3. **Models**: The "brain" created after training on data
4. **Prediction**: Using the trained model to make decisions on new data

Types of Machine Learning:
- **Supervised Learning**: Learning with labeled examples
- **Unsupervised Learning**: Finding patterns in unlabeled data  
- **Reinforcement Learning**: Learning through trial and error

Real-world applications include recommendation systems, image recognition, and natural language processing.

‚è±Ô∏è  Response time: 3.1s
```

**Why this works:** mistral excels at structured, educational explanations
</details>

### **Step 3: Experience the Before vs After Transformation**

**The Core MCP Demonstration:** This shows the exact problem MCP solves - wrong model selection leading to refusals.

<details>
<summary><strong>‚ùå The Problem: Manual Model Selection</strong></summary>

```bash
# First: Try manual model selection (the problem)
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "deepseek-coder-fast:latest", "prompt": "Plan a trip to Paris", "stream": false}'
```

**What happens:**
1. **Human Guessing:** Student manually chooses deepseek-coder-fast
2. **Wrong Specialization:** Coding model gets travel question
3. **Model Refusal:** Model recognizes it's not designed for this
4. **Failed Interaction:** Student gets refusal instead of help

**Actual Response:**
```json
{
  "model": "deepseek-coder-fast:latest",
  "response": "I'm sorry for the misunderstanding, but as an AI model developed by DeepSeek specifically designed towards answering computer science related queries and not suitable for planning travel itineraries, I can assist you only within this context.",
  "done": true
}
```

**The Problem:** Student wasted time and got no help with their actual question
</details>

<details>
<summary><strong>‚úÖ The Solution: MCP Smart Selection</strong></summary>

```bash
# Then: Use MCP smart selection (the solution)  
python3 smart_chat.py "Plan a trip to Paris"
```

**What happens:**
1. **Intelligent Analysis:** MCP detects "plan" and "trip" keywords
2. **Smart Routing:** Automatically selects phi-fast (travel-friendly model)
3. **Successful Match:** Right model gets right question type
4. **Helpful Response:** Student gets exactly what they needed

**MCP Response:**
```
ü§ñ Smart Chat - Automatic Model Selection
==================================================
üìù Question: Plan a trip to Paris

üß† Smart Analysis:
   ‚úì Category: travel_questions
   ‚úì Selected Model: phi-fast:latest
   ‚úì Reason: Detected travel keywords: ['plan', 'trip']

üîÑ Calling phi-fast:latest...
‚úÖ Smart Response:
--------------------
Here's a wonderful 3-day Paris itinerary:

Day 1: Classic Paris
- Morning: Visit the Eiffel Tower (arrive early to avoid crowds)
- Afternoon: Seine River cruise
- Evening: Dinner in the Latin Quarter

Day 2: Art & Culture  
- Morning: Louvre Museum (book timed entry)
- Afternoon: Walk through Tuileries Garden
- Evening: Montmartre and Sacr√©-C≈ìur

Day 3: Local Experience
- Morning: Local market visit (March√© Saint-Germain)
- Afternoon: Luxembourg Gardens
- Evening: Traditional bistro dinner

Tips: Book attractions in advance, carry a metro pass, learn basic French phrases!

‚è±Ô∏è  Response time: 2.1s
```

**The Value:** Student gets immediate, helpful response without any technical barriers
</details>
</details>

---

## üéØ Understanding the Magic

**Before MCP (Manual Selection):**
```bash
# Student manually tries different models - often gets wrong results
curl -X POST http://localhost:11434/api/generate \
  -d '{"model": "deepseek-coder-fast:latest", "prompt": "Plan a trip to Paris"}'
# Result: ‚ùå "I'm sorry... not suitable for travel itineraries"
```

**After MCP (Smart Selection):**
```bash
# MCP automatically analyzes and selects the right model
python3 smart_chat.py "Plan a trip to Paris"
# Result: ‚úÖ Helpful travel planning response
```

## üß† How Smart Routing Works

The `smart-mcp.json` file contains routing rules:

| Question Type | Keywords | Selected Model | Why |
|---------------|----------|----------------|-----|
| **Travel** | trip, travel, visit, vacation, plan | phi-fast:latest | General model good for travel |
| **Coding** | function, code, programming, debug | deepseek-coder-fast:latest | Specialized coding model |
| **General** | explain, what is, how to | mistral:latest | Good for explanations |

---

## üé® Customization

### Add New Routing Rules
Edit `smart-mcp.json` to add new question types:

```json
"new_question_type": {
  "keywords": ["your", "custom", "keywords"],
  "use_model": "your-preferred-model:latest",
  "avoid_model": "model-to-avoid:latest"
}
```

---

## üß™ Testing Your Setup

1. **Verify Models Available:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. **Test Individual Models:**
   ```bash
   # Test phi-fast
   curl -X POST http://localhost:11434/api/generate \
     -d '{"model": "phi-fast:latest", "prompt": "Hello", "stream": false}'
   
   # Test deepseek-coder-fast
   curl -X POST http://localhost:11434/api/generate \
     -d '{"model": "deepseek-coder-fast:latest", "prompt": "Hello", "stream": false}'
   ```

3. **Test Smart Routing:**
   ```bash
   # Different question types should route to different models
   python3 smart_chat.py "Plan a vacation"        # ‚Üí phi-fast
   python3 smart_chat.py "Write a function"       # ‚Üí deepseek-coder-fast
   python3 smart_chat.py "Explain recursion"      # ‚Üí mistral
   ```

---

## üö® Common Issues & Solutions

### Docker Issues:
```bash
# Permission denied error:
sudo usermod -aG docker $USER
newgrp docker

# Docker daemon not running:
sudo systemctl start docker

# Port conflicts:
sudo lsof -i :11434  # Check what's using port 11434
```

### Ollama Issues:
```bash
# Container not starting:
docker logs ollama  # Check container logs

# Models not downloading:
# Check internet connection and disk space
docker exec -it ollama ollama pull phi-fast:latest

# Clear cache and retry:
docker stop ollama
docker rm ollama
docker volume rm ollama
# Then run the docker run command again
```

---

## üìä Minimum System Requirements

| Component | Minimum | Recommended | Notes |
|-----------|---------|-------------|-------|
| **RAM** | 8GB | 16GB+ | Models load into RAM |
| **Storage** | 10GB free | 20GB+ | For models and Docker images |
| **CPU** | 4 cores | 8+ cores | More cores = faster inference |
| **Internet** | Stable | High-speed | For model downloads (6GB total) |
| **OS** | Linux/macOS/Windows | Linux preferred | Better Docker support |

---

## üéØ What You've Learned

- **Model Specialization:** Each model has specific strengths and weaknesses
- **Smart Routing:** Keywords automatically select the appropriate model
- **MCP Value:** Eliminates manual model selection and wrong choices
- **Automatic Coordination:** One question, right model, helpful response

## üîó Next Steps

- Try different question types and observe model selection
- Modify routing rules to match your specific needs
- Experiment with new models and routing strategies
- Experience the transformation from manual guessing to intelligent automation

**You've successfully built and tested an MCP-inspired smart routing system!** üéâ
